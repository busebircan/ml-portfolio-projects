"""
Network Churn Prediction
Predicts which customers will switch carriers based on behavior patterns
"""

import numpy as np
import pandas as pd
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import (
    roc_auc_score, confusion_matrix, precision_score, recall_score, f1_score
)
import joblib
import os

class NetworkChurnPredictor:
    \"\"\"\n    Predicts customer churn in telecom networks\n    \"\"\"\n    \n    def __init__(self):\n        self.model = None\n        self.scaler = StandardScaler()\n        self.label_encoders = {}\n        self.feature_names = None\n        self.feature_importance = None\n        \n    def generate_churn_data(self, n_samples=1000, churn_rate=0.2, random_state=42):\n        \"\"\"Generate synthetic churn data\"\"\"\n        np.random.seed(random_state)\n        \n        data = {\n            # Customer demographics\n            'age': np.random.uniform(18, 75, n_samples),\n            'tenure_months': np.random.exponential(20, n_samples),\n            'contract_type': np.random.choice(['Month-to-month', 'One year', 'Two year'], n_samples),\n            'internet_service': np.random.choice(['DSL', 'Fiber optic', 'No'], n_samples),\n            \n            # Usage patterns\n            'monthly_minutes': np.random.exponential(500, n_samples),\n            'data_usage_gb': np.random.exponential(50, n_samples),\n            'call_quality_score': np.random.uniform(1, 5, n_samples),\n            'network_satisfaction': np.random.uniform(1, 5, n_samples),\n            \n            # Service usage\n            'voice_calls_per_month': np.random.poisson(30, n_samples),\n            'sms_per_month': np.random.poisson(50, n_samples),\n            'data_sessions_per_day': np.random.poisson(10, n_samples),\n            \n            # Account information\n            'monthly_charges': np.random.uniform(20, 150, n_samples),\n            'total_charges': np.random.exponential(1000, n_samples),\n            'payment_method': np.random.choice(['Electronic check', 'Mailed check', 'Bank transfer', 'Credit card'], n_samples),\n            'paperless_billing': np.random.binomial(1, 0.7, n_samples),\n            \n            # Customer service interactions\n            'support_calls': np.random.poisson(2, n_samples),\n            'tech_support_used': np.random.binomial(1, 0.3, n_samples),\n            'complaint_count': np.random.poisson(0.5, n_samples),\n            'days_since_last_interaction': np.random.exponential(30, n_samples),\n            \n            # Network performance\n            'avg_latency_ms': np.random.exponential(50, n_samples),\n            'packet_loss_pct': np.random.uniform(0, 5, n_samples),\n            'dropped_calls_per_month': np.random.poisson(1, n_samples),\n            'network_outage_incidents': np.random.poisson(0.3, n_samples),\n            \n            # Competitive factors\n            'competitor_offers_received': np.random.poisson(2, n_samples),\n            'price_compared_to_market': np.random.uniform(-20, 20, n_samples),\n            'market_competitor_count': np.random.poisson(3, n_samples),\n        }\n        \n        df = pd.DataFrame(data)\n        \n        # Generate churn based on features\n        churn_probability = (\n            (1 - df['network_satisfaction'] / 5) * 0.2 +\n            (df['complaint_count'] / 5) * 0.15 +\n            (df['dropped_calls_per_month'] / 5) * 0.15 +\n            (df['price_compared_to_market'] / 100) * 0.1 +\n            (df['competitor_offers_received'] / 5) * 0.1 +\n            (df['support_calls'] / 10) * 0.1 +\n            (1 if df['contract_type'] == 'Month-to-month' else 0) * 0.2 +\n            (1 - df['tenure_months'] / 100) * 0.1\n        )\n        \n        churn_probability = np.clip(churn_probability, 0, 1)\n        df['churn'] = (np.random.random(n_samples) < churn_probability).astype(int)\n        \n        return df\n    \n    def preprocess_data(self, df, fit=True):\n        \"\"\"Preprocess data\"\"\"\n        df = df.copy()\n        \n        # Encode categorical variables\n        categorical_cols = df.select_dtypes(include=['object']).columns\n        for col in categorical_cols:\n            if fit:\n                self.label_encoders[col] = LabelEncoder()\n                df[col] = self.label_encoders[col].fit_transform(df[col])\n            else:\n                df[col] = self.label_encoders[col].transform(df[col])\n        \n        if fit:\n            self.feature_names = df.columns.tolist()\n            if 'churn' in self.feature_names:\n                self.feature_names.remove('churn')\n        \n        return df\n    \n    def train(self, df, test_size=0.2, random_state=42):\n        \"\"\"Train churn prediction model\"\"\"\n        print(\"Training Network Churn Prediction Model...\")\n        \n        # Preprocess\n        df_processed = self.preprocess_data(df, fit=True)\n        \n        # Prepare features and target\n        X = df_processed.drop('churn', axis=1)\n        y = df_processed['churn']\n        \n        # Split data\n        X_train, X_test, y_train, y_test = train_test_split(\n            X, y, test_size=test_size, random_state=random_state, stratify=y\n        )\n        \n        # Scale features\n        X_train_scaled = self.scaler.fit_transform(X_train)\n        X_test_scaled = self.scaler.transform(X_test)\n        \n        # Train model\n        print(\"  - Training Gradient Boosting Classifier...\")\n        self.model = GradientBoostingClassifier(\n            n_estimators=100,\n            learning_rate=0.1,\n            max_depth=5,\n            random_state=random_state\n        )\n        self.model.fit(X_train_scaled, y_train)\n        \n        # Feature importance\n        self.feature_importance = pd.DataFrame({\n            'feature': self.feature_names,\n            'importance': self.model.feature_importances_\n        }).sort_values('importance', ascending=False)\n        \n        # Evaluate\n        y_pred = self.model.predict(X_test_scaled)\n        y_pred_proba = self.model.predict_proba(X_test_scaled)[:, 1]\n        \n        metrics = {\n            'roc_auc': roc_auc_score(y_test, y_pred_proba),\n            'precision': precision_score(y_test, y_pred),\n            'recall': recall_score(y_test, y_pred),\n            'f1': f1_score(y_test, y_pred),\n            'confusion_matrix': confusion_matrix(y_test, y_pred)\n        }\n        \n        print(f\"    ROC-AUC: {metrics['roc_auc']:.4f}\")\n        print(f\"    Precision: {metrics['precision']:.4f}\")\n        print(f\"    Recall: {metrics['recall']:.4f}\")\n        print(f\"    F1-Score: {metrics['f1']:.4f}\")\n        \n        print(\"\\n  - Top 10 Churn Indicators:\")\n        for idx, row in self.feature_importance.head(10).iterrows():\n            print(f\"    {row['feature']}: {row['importance']:.4f}\")\n        \n        return metrics\n    \n    def predict_churn(self, features_df):\n        \"\"\"Predict churn probability\"\"\"\n        features_processed = self.preprocess_data(features_df, fit=False)\n        X = features_processed.drop('churn', axis=1, errors='ignore')\n        X_scaled = self.scaler.transform(X)\n        \n        churn_probability = self.model.predict_proba(X_scaled)[:, 1]\n        churn_class = self.model.predict(X_scaled)\n        \n        return churn_probability, churn_class\n    \n    def get_churn_risk_level(self, churn_probability):\n        \"\"\"Categorize churn risk\"\"\"\n        if churn_probability < 0.3:\n            return 'Low'\n        elif churn_probability < 0.6:\n            return 'Medium'\n        else:\n            return 'High'\n    \n    def save_model(self, model_path='models'):\n        \"\"\"Save model\"\"\"\n        os.makedirs(model_path, exist_ok=True)\n        joblib.dump(self.model, f'{model_path}/churn_model.pkl')\n        joblib.dump(self.scaler, f'{model_path}/scaler.pkl')\n        joblib.dump(self.label_encoders, f'{model_path}/label_encoders.pkl')\n        self.feature_importance.to_csv(f'{model_path}/feature_importance.csv', index=False)\n        print(f\"✓ Model saved to {model_path}/\")\n    \n    def load_model(self, model_path='models'):\n        \"\"\"Load model\"\"\"\n        self.model = joblib.load(f'{model_path}/churn_model.pkl')\n        self.scaler = joblib.load(f'{model_path}/scaler.pkl')\n        self.label_encoders = joblib.load(f'{model_path}/label_encoders.pkl')\n        self.feature_importance = pd.read_csv(f'{model_path}/feature_importance.csv')\n        print(f\"✓ Model loaded from {model_path}/\")\n\ndef main():\n    print(\"=\" * 60)\n    print(\"NETWORK CHURN PREDICTION\")\n    print(\"=\" * 60)\n    \n    # Generate data\n    predictor = NetworkChurnPredictor()\n    df = predictor.generate_churn_data(n_samples=1000, churn_rate=0.2)\n    \n    # Train model\n    print(\"\\n1. Training model...\")\n    metrics = predictor.train(df)\n    \n    # Save model\n    print(\"\\n2. Saving model...\")\n    predictor.save_model()\n    \n    print(\"\\n✓ Network Churn Prediction training completed successfully!\")\n\nif __name__ == '__main__':\n    main()\n
